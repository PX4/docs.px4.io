import{_ as t}from"./chunks/ref_frames.C52lwBbQ.js";import{_ as e,c as o,o as a,ab as r}from"./chunks/framework.CUflZczI.js";const b=JSON.parse('{"title":"Flying with Motion Capture (VICON, NOKOV, Optitrack)","description":"","frontmatter":{},"headers":[],"relativePath":"en/tutorials/motion-capture.md","filePath":"en/tutorials/motion-capture.md"}'),i={name:"en/tutorials/motion-capture.md"},n=r('<h1 id="flying-with-motion-capture-vicon-nokov-optitrack" tabindex="-1">Flying with Motion Capture (VICON, NOKOV, Optitrack) <a class="header-anchor" href="#flying-with-motion-capture-vicon-nokov-optitrack" aria-label="Permalink to &quot;Flying with Motion Capture (VICON, NOKOV, Optitrack)&quot;">​</a></h1><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p><strong>WORK IN PROGRESS</strong></p><p>This topic shares significant overlap with <a href="./../ros/external_position_estimation.html">External Position Estimation (ROS)</a>.</p></div><p>Indoor motion capture systems like VICON, NOKOV and Optitrack can be used to provide position and attitude data for vehicle state estimation, orto serve as ground-truth for analysis. The motion capture data can be used to update PX4&#39;s local position estimate relative to the local origin. Heading (yaw) from the motion capture system can also be optionally integrated by the attitude estimator.</p><p>Pose (position and orientation) data from the motion capture system is sent to the autopilot over MAVLink, using the <a href="https://mavlink.io/en/messages/common.html#ATT_POS_MOCAP" target="_blank" rel="noreferrer">ATT_POS_MOCAP</a> message. See the section below on coordinate frames for data representation conventions. The <a href="./../ros/mavros_installation.html">mavros</a> ROS-Mavlink interface has a default plugin to send this message. They can also be sent using pure C/C++ code and direct use of the MAVLink library.</p><h2 id="computing-architecture" tabindex="-1">Computing Architecture <a class="header-anchor" href="#computing-architecture" aria-label="Permalink to &quot;Computing Architecture&quot;">​</a></h2><p>It is <strong>highly recommended</strong> that you send motion capture data via an <strong>onboard</strong> computer (e.g Raspberry Pi, ODroid, etc.) for reliable communications. The onboard computer can be connected to the motion capture computer through WiFi, which offers reliable, high-bandwidth connection.</p><p>Most standard telemetry links like 3DR/SiK radios are <strong>not</strong> suitable for high-bandwidth motion capture applications.</p><h2 id="coordinate-frames" tabindex="-1">Coordinate Frames <a class="header-anchor" href="#coordinate-frames" aria-label="Permalink to &quot;Coordinate Frames&quot;">​</a></h2><p>This section shows how to setup the system with the proper reference frames. There are various representations but we will use two of them: ENU and NED.</p><ul><li>ENU is a ground-fixed frame where <strong>X</strong> axis points East, <strong>Y</strong> points North and <strong>Z</strong> up. The robot/vehicle body frame is <strong>X</strong> towards the front, <strong>Z</strong> up and <strong>Y</strong> towards the left.</li><li>NED has <strong>X</strong> towards North, <strong>Y</strong> East and <strong>Z</strong> down. The robot/vehicle body frame has <strong>X</strong> towards the front, <strong>Z</strong> down and <strong>Y</strong> accordingly.</li></ul><p>Frames are shown in the image below. NED on the left, ENU on the right:</p><p><img src="'+t+'" alt="Reference frames"></p><p>With the external heading estimation, however, magnetic North is ignored and faked with a vector corresponding to world <em>x</em> axis (which can be placed freely at mocap calibration); yaw angle will be given respect to local <em>x</em>.</p><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>When creating the rigid body in the motion capture software, remember to first align the robot with the world <strong>X</strong> axis otherwise yaw estimation will have an initial offset.</p></div><h2 id="estimator-choice" tabindex="-1">Estimator Choice <a class="header-anchor" href="#estimator-choice" aria-label="Permalink to &quot;Estimator Choice&quot;">​</a></h2><p>EKF2 is recommended for GPS-enabled systems (LPE is deprecated, and hence no longer supported or maintained). The Q-Estimator is recommended if you don&#39;t have GPS, as it works without a magnetometer or barometer.</p><p>See <a href="./../advanced/switching_state_estimators.html">Switching State Estimators</a> for more information.</p><h3 id="ekf2" tabindex="-1">EKF2 <a class="header-anchor" href="#ekf2" aria-label="Permalink to &quot;EKF2&quot;">​</a></h3><p>The ROS topic for motion cap <code>mocap_pose_estimate</code> for mocap systems and <code>vision_pose_estimate</code> for vision. Check <a href="http://wiki.ros.org/mavros_extras" target="_blank" rel="noreferrer">mavros_extras</a> for further info.</p><h2 id="testing" tabindex="-1">Testing <a class="header-anchor" href="#testing" aria-label="Permalink to &quot;Testing&quot;">​</a></h2><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">​</a></h2>',21),s=[n];function h(c,l,d,m,p,g){return a(),o("div",null,s)}const _=e(i,[["render",h]]);export{b as __pageData,_ as default};
