import{_ as e}from"./chunks/ref_frames.YBU277-S.js";import{_ as o,c as a,a8 as r,o as i}from"./chunks/framework.BDnHobkS.js";const g=JSON.parse('{"title":"使用 Motion Capture 飞行（VICON，Optitrack）","description":"","frontmatter":{},"headers":[],"relativePath":"zh/tutorials/motion-capture.md","filePath":"zh/tutorials/motion-capture.md"}'),n={name:"zh/tutorials/motion-capture.md"};function s(c,t,h,l,d,m){return i(),a("div",null,t[0]||(t[0]=[r('<h1 id="使用-motion-capture-飞行-vicon-optitrack" tabindex="-1">使用 Motion Capture 飞行（VICON，Optitrack） <a class="header-anchor" href="#使用-motion-capture-飞行-vicon-optitrack" aria-label="Permalink to &quot;使用 Motion Capture 飞行（VICON，Optitrack）&quot;">​</a></h1><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p><strong>WORK IN PROGRESS</strong></p><p>This topic shares significant overlap with <a href="./../ros/external_position_estimation.html">External Position Estimation (ROS)</a>.</p></div><p>Indoor motion capture systems like VICON, NOKOV and Optitrack can be used to provide position and attitude data for vehicle state estimation, orto serve as ground-truth for analysis. The motion capture data can be used to update PX4&#39;s local position estimate relative to the local origin. Heading (yaw) from the motion capture system can also be optionally integrated by the attitude estimator.</p><p>Pose (position and orientation) data from the motion capture system is sent to the autopilot over MAVLink, using the <a href="https://mavlink.io/en/messages/common.html#ATT_POS_MOCAP" target="_blank" rel="noreferrer">ATT_POS_MOCAP</a> message. See the section below on coordinate frames for data representation conventions. The <a href="./../ros/mavros_installation.html">mavros</a> ROS-Mavlink interface has a default plugin to send this message. They can also be sent using pure C/C++ code and direct use of the MAVLink library.</p><h2 id="computing-architecture" tabindex="-1">Computing Architecture <a class="header-anchor" href="#computing-architecture" aria-label="Permalink to &quot;Computing Architecture&quot;">​</a></h2><p>It is <strong>highly recommended</strong> that you send motion capture data via an <strong>onboard</strong> computer (e.g Raspberry Pi, ODroid, etc.) for reliable communications. The onboard computer can be connected to the motion capture computer through WiFi, which offers reliable, high-bandwidth connection.</p><p>Most standard telemetry links like 3DR/SiK radios are <strong>not</strong> suitable for high-bandwidth motion capture applications.</p><h2 id="coordinate-frames" tabindex="-1">Coordinate Frames <a class="header-anchor" href="#coordinate-frames" aria-label="Permalink to &quot;Coordinate Frames&quot;">​</a></h2><p>This section shows how to setup the system with the proper reference frames. There are various representations but we will use two of them: ENU and NED.</p><ul><li>ENU is a ground-fixed frame where <strong>X</strong> axis points East, <strong>Y</strong> points North and <strong>Z</strong> up. The robot/vehicle body frame is <strong>X</strong> towards the front, <strong>Z</strong> up and <strong>Y</strong> towards the left.</li><li>NED has <strong>X</strong> towards North, <strong>Y</strong> East and <strong>Z</strong> down. The robot/vehicle body frame has <strong>X</strong> towards the front, <strong>Z</strong> down and <strong>Y</strong> accordingly.</li></ul><p>Frames are shown in the image below. NED on the left, ENU on the right:</p><p><img src="'+e+'" alt="Reference frames"></p><p>With the external heading estimation, however, magnetic North is ignored and faked with a vector corresponding to world <em>x</em> axis (which can be placed freely at mocap calibration); yaw angle will be given respect to local <em>x</em>.</p><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>When creating the rigid body in the motion capture software, remember to first align the robot with the world <strong>X</strong> axis otherwise yaw estimation will have an initial offset.</p></div><h2 id="estimator-choice" tabindex="-1">Estimator Choice <a class="header-anchor" href="#estimator-choice" aria-label="Permalink to &quot;Estimator Choice&quot;">​</a></h2><p>EKF2 is recommended for GPS-enabled systems (LPE is deprecated, and hence no longer supported or maintained). The Q-Estimator is recommended if you don&#39;t have GPS, as it works without a magnetometer or barometer.</p><p>See <a href="./../advanced/switching_state_estimators.html">Switching State Estimators</a> for more information.</p><h3 id="ekf2" tabindex="-1">EKF2 <a class="header-anchor" href="#ekf2" aria-label="Permalink to &quot;EKF2&quot;">​</a></h3><p>The ROS topic for motion cap <code>mocap_pose_estimate</code> for mocap systems and <code>vision_pose_estimate</code> for vision. Check <a href="http://wiki.ros.org/mavros_extras" target="_blank" rel="noreferrer">mavros_extras</a> for further info.</p><h2 id="测试" tabindex="-1">测试 <a class="header-anchor" href="#测试" aria-label="Permalink to &quot;测试&quot;">​</a></h2><h2 id="故障处理" tabindex="-1">故障处理 <a class="header-anchor" href="#故障处理" aria-label="Permalink to &quot;故障处理&quot;">​</a></h2>',21)]))}const f=o(n,[["render",s]]);export{g as __pageData,f as default};
